{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 Slices.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VkWsgPKcws-Q-kZsaw6Vx4J5w_mW62oh",
      "authorship_tag": "ABX9TyO8qkDMkWL+yt5IA64VV5d+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Basantallam/MachineLearning-Segmentation/blob/main/5_Slices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JocJd3rSo-lW",
        "outputId": "861d33a4-6b6f-42b9-b352-dfb9d8ee7df2"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import nibabel\r\n",
        "\r\n",
        "data_path = '//content//drive//MyDrive'\r\n",
        "# we will undersample our training 2D images later (for memory and speed)\r\n",
        "\r\n",
        "# rows and cols /2 because they will be undersampled by 2 later\r\n",
        "image_rows = int(182//2)  \r\n",
        "image_cols = int(219//2) \r\n",
        "\r\n",
        "\r\n",
        "def create_train_data():\r\n",
        "    print('-'*30)\r\n",
        "    print('Creating training data...')\r\n",
        "    print('-'*30)\r\n",
        "    #listing names of all training data into an array of strings\r\n",
        "    #1st array is for input and the 2nd for labels (masks)\r\n",
        "    train_data_path = os.path.join(data_path, 'train//train')\r\n",
        "    training_images = os.listdir(train_data_path)\r\n",
        "\r\n",
        "    train_mask_data_path = os.path.join(data_path, 'train//train_masks')\r\n",
        "    training_masks = os.listdir(train_mask_data_path)\r\n",
        "\r\n",
        "    #sorting file names alphabetically so they are correctly ordered  \r\n",
        "    list.sort(training_images)\r\n",
        "    list.sort(training_masks)\r\n",
        "\r\n",
        "    #since we are doing 2.5D we will only use specific 2D images from the 3D image\r\n",
        "    #the chosen 2D images will be appended to the following two arrays:\r\n",
        "\r\n",
        "    #training images\r\n",
        "    imgs_train = [] \r\n",
        "    #training masks \r\n",
        "    masks_train = [] \r\n",
        "\r\n",
        "    #looping over names of 3D images and extracting 2D images from them\r\n",
        "    for i in range(0,len(training_images)):\r\n",
        "      \r\n",
        "        # every patient has 4 mri modalities with only 1 mask \r\n",
        "        # so \"i\" is divided by 4 in training_masks[i//4] so the same mask \r\n",
        "        # is used for with its 4 corresponding mri scans\r\n",
        "        # we load 3D training masks\r\n",
        "        training_mask = nibabel.load(os.path.join(train_mask_data_path,training_masks[i//4]))\r\n",
        "        #we load 3D training image\r\n",
        "        training_image = nibabel.load(os.path.join(train_data_path, training_images[i])) \r\n",
        "        \r\n",
        "        #3D image is converted to a 3D numpy array\r\n",
        "        #k is the index you  take the 2D image at.\r\n",
        "        #In this approach we took 4 consecutive 2D images from the center of the 3Dimage\r\n",
        "        for k in range((training_mask.shape[2]//2)-2,(training_mask.shape[2]//2)+2):\r\n",
        "            #axial cuts are made along the z axis with undersampling by 2\r\n",
        "            mask_2d = np.array(training_mask.get_fdata()[::2, ::2, k]) \r\n",
        "            image_2d = np.array(training_image.get_fdata()[::2, ::2, k])\r\n",
        "           # print(\"k=\"+str(k))\r\n",
        "           #array of chosen 2D images of all patients appended together \r\n",
        "           #(Array of 2D arrays)\r\n",
        "            masks_train.append(mask_2d)\r\n",
        "            imgs_train.append(image_2d)\r\n",
        "   \r\n",
        "    #creating an empty numpy array \r\n",
        "        \r\n",
        "    imgs = np.ndarray((len(imgs_train), image_rows, image_cols), dtype=np.uint8)\r\n",
        "    imgs_mask = np.ndarray((len(masks_train), image_rows, image_cols), dtype=np.uint8)\r\n",
        "\r\n",
        "\r\n",
        "    # converting from list to numpy array\r\n",
        "    for index, img in enumerate(imgs_train):\r\n",
        "        imgs[index, :, :] = img\r\n",
        "    # converting from list to numpy array\r\n",
        "    for index, img in enumerate(masks_train):\r\n",
        "        imgs_mask[index, :, :] = img\r\n",
        "\r\n",
        "    #saving numpy array file   \r\n",
        "    np.save('/content/drive/MyDrive/numpy array files 2.5D/5 slices in same plane//imgs_train.npy', imgs)\r\n",
        "    np.save('/content/drive/MyDrive/numpy array files 2.5D/5 slices in same plane//masks_train.npy', imgs_mask)\r\n",
        "\r\n",
        "    print('Saving training data to .npy files done.')\r\n",
        "\r\n",
        "def create_test_data():\r\n",
        "    print('-'*30)\r\n",
        "    print('Creating test data...')\r\n",
        "    print('-'*30)\r\n",
        "    test_data_path = '//content//drive//MyDrive//test'\r\n",
        "\r\n",
        "    tt=os.path.join(test_data_path, 'test')\r\n",
        "    test_mask_path = os.path.join(test_data_path, 'mask')\r\n",
        "\r\n",
        "    imgs_test = os.listdir(tt)     \r\n",
        "    masks_test = os.listdir(test_mask_path) \r\n",
        "\r\n",
        "    #sorting file names alphabetically so they are correctly ordered  \r\n",
        "    list.sort(imgs_test)\r\n",
        "    list.sort(masks_test)\r\n",
        "\r\n",
        "     \r\n",
        "\r\n",
        "    #looping over names of 3D images and extracting 2D images from them\r\n",
        "    for i in range(len(imgs_test)):\r\n",
        "       # every patient has 3 mri modalities with only 1 mask \r\n",
        "        # so \"i\" is divided by 3 in masks_test[i//3] so the same mask \r\n",
        "        # is used for with its 3 corresponding mri scans\r\n",
        "\r\n",
        "      \r\n",
        "        # we load 3D training masks\r\n",
        "        img = nibabel.load(os.path.join(tt,imgs_test[i]))\r\n",
        "        msk = nibabel.load(os.path.join(test_mask_path ,  masks_test[i//3]))\r\n",
        "\r\n",
        "    #since we are doing 2.5D we will only use specific 2D images from the 3D image\r\n",
        "    #the chosen 2D images will be appended to the following two arrays:\r\n",
        "\r\n",
        "        imgs_testnpy=[]\r\n",
        "        masks_testnpy=[]\r\n",
        "          \r\n",
        "        # 3D image is converted to a 3D numpy array\r\n",
        "        # k is the index you  take the 2D image at.\r\n",
        "        # In this approach we took 4 consecutive 2D images from the center of the 3Dimage\r\n",
        "        for k in range((img.shape[2]//2)-2,(img.shape[2]//2)+2):\r\n",
        "           # print(\"k=\",k)\r\n",
        "            #axial cuts are made along the z axis with undersampling by 2\r\n",
        "            img_2d = np.array(img.get_fdata()[::2, ::2, k])\r\n",
        "            msk_2d = np.array(msk.get_fdata()[::2, ::2, k])\r\n",
        "\r\n",
        "            # array of chosen 2D images of all patients appended together \r\n",
        "            #(Array of 2D arrays)\r\n",
        "            imgs_testnpy.append(img_2d)\r\n",
        "            masks_testnpy.append(msk_2d)\r\n",
        "\r\n",
        "    # creating an empty numpy array \r\n",
        "    imgst = np.ndarray((len(imgs_testnpy), image_rows, image_cols), dtype=np.uint8)\r\n",
        "    imgs_maskt = np.ndarray((len(masks_testnpy), image_rows, image_cols), dtype=np.uint8)\r\n",
        "    \r\n",
        "    # converting from list to numpy array\r\n",
        "    for index, img in enumerate(imgs_testnpy):\r\n",
        "        imgst[index, :, :] = img\r\n",
        "    \r\n",
        "    # converting from list to numpy array\r\n",
        "    for index, img in enumerate(masks_testnpy):\r\n",
        "        imgs_maskt[index, :, :] = img\r\n",
        "\r\n",
        "    #saving numpy array file   \r\n",
        "    np.save('/content/drive/MyDrive/numpy array files 2.5D/5 slices in same plane/imgs_test.npy', imgst)\r\n",
        "    np.save('/content/drive/MyDrive/numpy array files 2.5D/5 slices in same plane/masks_test.npy', imgs_maskt)\r\n",
        "\r\n",
        "    print('Saving test data to .npy files done.')\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    create_train_data()\r\n",
        "    create_test_data()\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Creating training data...\n",
            "------------------------------\n",
            "Saving training data to .npy files done.\n",
            "------------------------------\n",
            "Creating test data...\n",
            "------------------------------\n",
            "Saving test data to .npy files done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyv59m8Rtm0-",
        "outputId": "767f3d1a-aff5-4189-b11c-12a44d40f138"
      },
      "source": [
        "pip install SimpleITK"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SimpleITK\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/85/6a7ce61f07cdaca722dd64f028b5678fb0a9e1bf66f534c2f8dd2eb78490/SimpleITK-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 66kB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hECBm2yzt3bO",
        "outputId": "05bddab0-f94c-4b88-dfe0-9e3076f19513"
      },
      "source": [
        "pip install medpy"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting medpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/70/c1fd5dd60242eee81774696ea7ba4caafac2bad8f028bba94b1af83777d7/MedPy-0.4.0.tar.gz (151kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 27.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 30.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51kB 29.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 32.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 20.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 19.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 20.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102kB 19.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 19.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 122kB 19.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 19.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 19.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.19.5)\n",
            "Requirement already satisfied: SimpleITK>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (2.0.2)\n",
            "Building wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.4.0-cp36-cp36m-linux_x86_64.whl size=753436 sha256=16d3dbea9c37c2e84def6337d3d3d37bd4bea1897baf90496916097e6ad60ecf\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/c9/9c/2c6281c7a72b9fb1570862a4f028af7ce38405008354fbf870\n",
            "Successfully built medpy\n",
            "Installing collected packages: medpy\n",
            "Successfully installed medpy-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X3DsqM-cpEPt",
        "outputId": "81967391-1109-4656-fecb-19134fb65a19"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "\r\n",
        "import os\r\n",
        "from skimage.transform import resize\r\n",
        "from skimage.io import imsave\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from skimage.segmentation import mark_boundaries\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\r\n",
        "from keras.optimizers import Adam, SGD\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras import backend as K\r\n",
        "from skimage.exposure import rescale_intensity\r\n",
        "from keras.callbacks import History\r\n",
        "from skimage import io\r\n",
        "from load_ import load_train_data, load_test_data\r\n",
        "import SimpleITK as sitk\r\n",
        "import nibabel as nib \r\n",
        "from medpy.io import load, save\r\n",
        "# powers of 2 are preferred in rows and cols so in down convolution we can split \r\n",
        "#the image dimensions by 2 several times without getting fractions\r\n",
        "image_rows = int(128) \r\n",
        "image_cols = int(128) \r\n",
        "\r\n",
        "smooth = 1.\r\n",
        "#We divide here the number of rows and columns by two because we undersample our data (We take one pixel over two)\r\n",
        "\r\n",
        "def dice_coef(y_true, y_pred):\r\n",
        "    y_true_f = K.flatten(y_true)\r\n",
        "    y_pred_f = K.flatten(y_pred)\r\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\r\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\r\n",
        "\r\n",
        "def dice_coef_loss(y_true, y_pred):\r\n",
        "    return -dice_coef(y_true, y_pred)\r\n",
        "\r\n",
        "def get_unet():\r\n",
        "    #down-convolution\r\n",
        "    inputs = Input((image_rows, image_cols, 1))\r\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\r\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\r\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n",
        "    \r\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\r\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\r\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n",
        "   \r\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\r\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\r\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n",
        "    \r\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\r\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\r\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\r\n",
        "\r\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\r\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\r\n",
        "    \r\n",
        "    #up-convolution:\r\n",
        "    \r\n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\r\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\r\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\r\n",
        "\r\n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\r\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\r\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\r\n",
        "\r\n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\r\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\r\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\r\n",
        "\r\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\r\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\r\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\r\n",
        "\r\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\r\n",
        "\r\n",
        "    model = Model(inputs=[inputs], outputs=[conv10])\r\n",
        "\r\n",
        "    model.compile(optimizer=Adam(lr=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "#The different layers in our neural network model (including convolutions, maxpooling and upsampling)\r\n",
        "\r\n",
        "def preprocess(imgs):\r\n",
        "  #We adapt here our dataset samples dimension to 128x128 so that we can feed it to our network\r\n",
        "    imgs_p = np.ndarray((imgs.shape[0], image_rows, image_cols), dtype=np.uint8)\r\n",
        "    for i in range(imgs.shape[0]):\r\n",
        "        imgs_p[i] = resize(imgs[i], (image_rows, image_cols), preserve_range=True)\r\n",
        "\r\n",
        "    imgs_p = imgs_p[..., np.newaxis]\r\n",
        "    return imgs_p\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def train_and_predict():\r\n",
        "    print('-'*30)\r\n",
        "    print('Loading and preprocessing train data...')\r\n",
        "    print('-'*30)\r\n",
        "    \r\n",
        "    imgs_train, imgs_mask_train = load_train_data()\r\n",
        "\r\n",
        "    imgs_train = preprocess(imgs_train)\r\n",
        "    imgs_mask_train = preprocess(imgs_mask_train)\r\n",
        "\r\n",
        "    imgs_train = imgs_train.astype('float32')\r\n",
        "    mean = np.mean(imgs_train)  # mean for data centering\r\n",
        "    std = np.std(imgs_train)  # std for data normalization\r\n",
        "\r\n",
        "    imgs_train -= mean\r\n",
        "    imgs_train /= std\r\n",
        "    #Normalization of the train set\r\n",
        "\r\n",
        "    imgs_mask_train = imgs_mask_train.astype('float32')\r\n",
        "\r\n",
        "    print('-'*30)\r\n",
        "    print('Creating and compiling model...')\r\n",
        "    print('-'*30)\r\n",
        "    model = get_unet()\r\n",
        "    model_checkpoint = ModelCheckpoint('weights.h5', monitor='val_loss', save_best_only=True)\r\n",
        "    #Saving the weights and the loss of the best predictions we obtained\r\n",
        "\r\n",
        "    print('-'*30)\r\n",
        "    print('Fitting model...')\r\n",
        "    print('-'*30)\r\n",
        "\r\n",
        "    history=model.fit(imgs_train, imgs_mask_train, batch_size=1, epochs=50, verbose=1, shuffle=True,\r\n",
        "              validation_split=0.7,callbacks=[model_checkpoint])\r\n",
        "\r\n",
        "    print('-'*30)\r\n",
        "    print('Loading and preprocessing test data...')\r\n",
        "    print('-'*30)\r\n",
        "\r\n",
        "    imgs_test, imgs_maskt = load_test_data()\r\n",
        "    \r\n",
        "    imgs_test = preprocess(imgs_test)\r\n",
        "\r\n",
        "    imgs_test = imgs_test.astype('float32')\r\n",
        "    imgs_test -= mean\r\n",
        "    imgs_test /= std\r\n",
        "    #Normalization of the test set\r\n",
        "\r\n",
        "    print('-'*30)\r\n",
        "    print('Loading saved weights...')\r\n",
        "    print('-'*30)\r\n",
        "    model.load_weights('weights.h5')\r\n",
        "\r\n",
        "    print('-'*30)\r\n",
        "    print('Predicting masks on test data...')\r\n",
        "    print('-'*30)\r\n",
        "    \r\n",
        "    pred_mask_test = model.predict(imgs_test, verbose=1)\r\n",
        "    print(np.unique(pred_mask_test),\" u\")\r\n",
        "\r\n",
        "    np.save('pred_mask_test.npy', pred_mask_test)\r\n",
        "\r\n",
        "    print(type(pred_mask_test))\r\n",
        "\r\n",
        "    print('-' * 30)\r\n",
        "    print('Saving predicted masks to files...')\r\n",
        "    print('-' * 30)\r\n",
        "    pred_dir = '/content/drive/MyDrive/train'\r\n",
        "\r\n",
        "    #for k in range(len(pred_mask_test)):\r\n",
        "    #  a = rescale_intensity(imgs_test[k][:,:,0],out_range=(-1,1))\r\n",
        "    #  b = (pred_mask_test[k][:,:,0]).astype('uint8')\r\n",
        "    #  io.imsave(os.path.join(pred_dir, str(k) + '_pred.png'),mark_boundaries(a,b))\r\n",
        "\r\n",
        "    #Saving our predictions in the directory 'preds'\r\n",
        "    plt.plot(history.history['dice_coef'])\r\n",
        "    plt.plot(history.history['val_dice_coef'])\r\n",
        "    plt.title('Model dice coeff')\r\n",
        "    plt.ylabel('Dice coeff')\r\n",
        "    plt.xlabel('Epoch')\r\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "    plt.show()\r\n",
        "    #plotting our dice coeff results in function of the number of epochs\r\n",
        "\r\n",
        "    results=model.evaluate(x=imgs_test,y=pred_mask_test,batch_size=None,\r\n",
        "    verbose=1,sample_weight=None,steps=None,callbacks=None,\r\n",
        "    max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False)\r\n",
        "    print(\"test loss, test acc:\", results)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    train_and_predict()\r\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Loading and preprocessing train data...\n",
            "------------------------------\n",
            "------------------------------\n",
            "Creating and compiling model...\n",
            "------------------------------\n",
            "------------------------------\n",
            "Fitting model...\n",
            "------------------------------\n",
            "Epoch 1/50\n",
            "144/144 [==============================] - 4s 22ms/step - loss: -0.2051 - dice_coef: 0.2051 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 2/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2724 - dice_coef: 0.2724 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 3/50\n",
            "144/144 [==============================] - 3s 21ms/step - loss: -0.2591 - dice_coef: 0.2591 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 4/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2784 - dice_coef: 0.2784 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 5/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2807 - dice_coef: 0.2807 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 6/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3567 - dice_coef: 0.3567 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 7/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2436 - dice_coef: 0.2436 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 8/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3518 - dice_coef: 0.3518 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 9/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3513 - dice_coef: 0.3513 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 10/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2822 - dice_coef: 0.2822 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 11/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2861 - dice_coef: 0.2861 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 12/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2751 - dice_coef: 0.2751 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 13/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2886 - dice_coef: 0.2886 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 14/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3252 - dice_coef: 0.3252 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 15/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2291 - dice_coef: 0.2291 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 16/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3494 - dice_coef: 0.3494 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 17/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2688 - dice_coef: 0.2688 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 18/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2945 - dice_coef: 0.2945 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 19/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3159 - dice_coef: 0.3159 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 20/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3506 - dice_coef: 0.3506 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 21/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2557 - dice_coef: 0.2557 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 22/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2376 - dice_coef: 0.2376 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 23/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2417 - dice_coef: 0.2417 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 24/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2929 - dice_coef: 0.2929 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 25/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2829 - dice_coef: 0.2829 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 26/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2508 - dice_coef: 0.2508 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 27/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3205 - dice_coef: 0.3205 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 28/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2398 - dice_coef: 0.2398 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 29/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2558 - dice_coef: 0.2558 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 30/50\n",
            "144/144 [==============================] - 3s 21ms/step - loss: -0.3097 - dice_coef: 0.3097 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 31/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2871 - dice_coef: 0.2871 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 32/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2298 - dice_coef: 0.2298 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 33/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2618 - dice_coef: 0.2618 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 34/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3062 - dice_coef: 0.3062 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 35/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2287 - dice_coef: 0.2287 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 36/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2939 - dice_coef: 0.2939 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 37/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2394 - dice_coef: 0.2394 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 38/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2541 - dice_coef: 0.2541 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 39/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2730 - dice_coef: 0.2730 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 40/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3002 - dice_coef: 0.3002 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 41/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2439 - dice_coef: 0.2439 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 42/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2931 - dice_coef: 0.2931 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 43/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2444 - dice_coef: 0.2444 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 44/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3544 - dice_coef: 0.3544 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 45/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3396 - dice_coef: 0.3396 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 46/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2884 - dice_coef: 0.2884 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 47/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2586 - dice_coef: 0.2586 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 48/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2929 - dice_coef: 0.2929 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 49/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.3146 - dice_coef: 0.3146 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "Epoch 50/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: -0.2920 - dice_coef: 0.2920 - val_loss: -0.4394 - val_dice_coef: 0.4394\n",
            "------------------------------\n",
            "Loading and preprocessing test data...\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading saved weights...\n",
            "------------------------------\n",
            "------------------------------\n",
            "Predicting masks on test data...\n",
            "------------------------------\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6e7f4900d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "[0.]  u\n",
            "<class 'numpy.ndarray'>\n",
            "------------------------------\n",
            "Saving predicted masks to files...\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 308ms/step - loss: -1.0000 - dice_coef: 1.0000\n",
            "test loss, test acc: [-1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}